{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import bs4\n",
    "import os\n",
    "import sqlite3\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set SCRAPE=True in order to download the data from FB\n",
    "### Set REDUCE=True to reduce the separated json files to one\n",
    "### SCRAPE and REDUCE must not be both set to True or the script will crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCRAPE = True\n",
    "\n",
    "UNIFIED_JSON_PATH = \"posts.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to return an empty list if string is not JSON formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    try:\n",
    "        return json.loads(open(path, \"r\").read())\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        return []\n",
    "\n",
    "def json_append(path, post):\n",
    "    data = read_json(path)\n",
    "    data.append(post)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a supplemental function to simplify API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get(url_):\n",
    "    return json.loads(requests.get(url_).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define gruop ID variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TECHNION_CONFESSIONS_ID = \"134517547222780\"\n",
    "TAU_CONFESSIONS_ID = \"561380070875128\"\n",
    "IDC_CONFESSIONS_ID = \"199527394120566\"\n",
    "HUJI_CONFESSIONS_ID = \"323288791493138\"\n",
    "BGU_CONFESSIONS_ID = \"151003595697352\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to Graph API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TOKEN = \"EAACEdEose0cBAEIYEOmJ51kfOHPinwL2ychqVpFT0D9ezMjfe1pMKyH0lP2ZA9RuZBH3wQeJ0EfEboHEqDqy7Gq8PSjIcHgNv1lRyC3ZBMk0CdbdzWckHsqNTRatwPZBvkMBfic2P1jwg5vZAZCHKbHdvFfMjjsHGjeVsY7AqEbpWiUBEb0d1zsKJgJwQw3wTWs6ZBb4WSOOQZDZD\"\n",
    "\n",
    "s = \"6cc937f2a9dbc9df92600f365c777d1a\"\n",
    "i = \"652869818252649\"\n",
    "u = \"https://graph.facebook.com/oauth/access_token?client_id={id}&client_secret={secret}&grant_type=client_credentials\"\n",
    "\n",
    "TOKEN = get(u.format(id=i, secret=s))['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Graph API host and API node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOST = \"https://graph.facebook.com/\"\n",
    "API_NODE = \"v2.12/\"\n",
    "QUERY = \"?fields=created_time,message_tags,message,shares,reactions.type(LIKE).limit(0).summary(1).as(like),reactions.type(LOVE).limit(0).summary(1).as(love),reactions.type(HAHA).limit(0).summary(1).as(haha),reactions.type(WOW).limit(0).summary(1).as(wow),reactions.type(SAD).limit(0).summary(1).as(sad),reactions.type(ANGRY).limit(0).summary(1).as(angry)&limit=10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of the pages that are going to be scraped in a manner of (id, alias) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAGES = [\n",
    "    (TECHNION_CONFESSIONS_ID, \"TECHNION\"),\n",
    "    (TAU_CONFESSIONS_ID, \"TAU\"),\n",
    "    (IDC_CONFESSIONS_ID, \"IDC\"),\n",
    "    (HUJI_CONFESSIONS_ID, \"HUJI\"),\n",
    "    (BGU_CONFESSIONS_ID, \"BGU\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Container for the scraped posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a supplemental builder functions for the API calls' URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_url(page_id):\n",
    "    return HOST + API_NODE + page_id + \"/posts\" + QUERY + \"&access_token={}\".format(TOKEN)\n",
    "\n",
    "def build_comments_url(post_id):\n",
    "    return HOST + API_NODE + post_id + \"/comments\" + QUERY + \"&access_token={}\".format(TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that scrapes a general object (post or comment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_object(raw_object, object_type, object_source, parent_id=None):\n",
    "    obj = {}   \n",
    "    \n",
    "    obj['type'] = object_type\n",
    "    obj['source'] = object_source\n",
    "    \n",
    "    obj['id'] = raw_object['id']\n",
    "    obj['parent_id'] = parent_id\n",
    "    \n",
    "    try:\n",
    "        obj['message'] = raw_object['message']\n",
    "    except KeyError:\n",
    "        obj['message'] = ''\n",
    "\n",
    "    obj['created_time'] = raw_object['created_time']\n",
    "\n",
    "    like = raw_object[\"like\"][\"summary\"][\"total_count\"]\n",
    "    love = raw_object[\"love\"][\"summary\"][\"total_count\"]\n",
    "    haha = raw_object[\"haha\"][\"summary\"][\"total_count\"]\n",
    "    wow = raw_object[\"wow\"][\"summary\"][\"total_count\"]\n",
    "    sad = raw_object[\"sad\"][\"summary\"][\"total_count\"]\n",
    "    angry = raw_object[\"angry\"][\"summary\"][\"total_count\"]\n",
    "\n",
    "    obj['like'] = like\n",
    "    obj['love'] = love\n",
    "    obj['haha'] = haha\n",
    "    obj['wow'] = wow\n",
    "    obj['sad'] = sad\n",
    "    obj['angry'] = angry\n",
    "    \n",
    "    obj['total_reactions'] = sum([like, love, haha, wow, sad, angry])\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a fumction that will paginate through the posts of a URL and scrape them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posts(url, source):\n",
    "    res = get(url)\n",
    "    if 'data' in res:\n",
    "        for i, raw_post in enumerate(res['data']):\n",
    "            post = query_object(raw_post, \"POST\", source)\n",
    "            posts.append(post)\n",
    "            if len(posts) % 250 == 0:\n",
    "                print(\"{} is on item #{}\".format(source, len(posts)))\n",
    "    if 'paging' in res:\n",
    "        if 'next' in res['paging']:\n",
    "            return get_posts(res['paging']['next'], source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will scrape a tag of a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_tag(raw_tag, comment_id, source):\n",
    "    tag = {}\n",
    "    \n",
    "    tag[\"type\"] = \"TAG\"\n",
    "    tag[\"parent_id\"] = comment_id\n",
    "    tag[\"source\"] = source\n",
    "    \n",
    "    tag[\"user_id\"] = raw_tag[\"id\"]\n",
    "    tag[\"user_name\"] = raw_tag[\"name\"]\n",
    "    \n",
    "    return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will call the API for each post comments, scrape them and get their tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_post_comments(url, post_id, post_source):\n",
    "    json_path = \"{}.json\".format(post_source)\n",
    "    res = get(url)\n",
    "    if 'data' in res:\n",
    "        for raw_comment in res['data']:\n",
    "            comment = query_object(raw_comment, \"COMMENT\", post_source, parent_id=post_id)\n",
    "            posts.append(comment)\n",
    "            \n",
    "            if raw_comment.get(\"message_tags\"):\n",
    "                for raw_tag in raw_comment.get(\"message_tags\"):\n",
    "                    tag = parse_tag(raw_tag, raw_comment[\"id\"], post_source)\n",
    "                    posts.append(tag)\n",
    "            \n",
    "            if len(posts) % 250 == 0:\n",
    "                print(\"{} is on item #{}\".format(post_source, len(posts)))\n",
    "            \n",
    "    if 'paging' in res:\n",
    "        if 'next' in res['paging']:\n",
    "            return get_post_comments(res['paging']['next'], post_id, post_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, create a function that will scrapes all of the posts' comments and comment tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comments():\n",
    "    for post in posts:\n",
    "        get_post_comments(build_comments_url(post[\"id\"]), post[\"id\"], post[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use threading to query each Facebook Group in a different thread - this is the worker funciton:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If SCRAPE, the script will run the threads (stop them using >>taskkill /f /im -\"python.exe\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if SCRAPE:\n",
    "    for node, name in PAGES:\n",
    "        print(\"[I] Scraping posts of {}...\".format(name))\n",
    "        get_posts(build_url(node), name)\n",
    "        print(\"{} is done.\".format(name))\n",
    "        \n",
    "    print(\"[II] Scraping comments...\".format(name))\n",
    "    get_comments(\"{}.json\".format(name))\n",
    "\n",
    "    print(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"data.db\")\n",
    "df.to_sql(\"posts\", conn, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ID_REGEX = \"(#\\d+|\\d+#){1}\"\n",
    "# df[\"id\"] = df[\"message\"].str.findall(ID_REGEX).str.get(0)\n",
    "# df[\"reply_to\"] = df[\"message\"].str.findall(ID_REGEX).str.get(1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.groupby([\"source\"]).agg([\"count\"])[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ID of the posts that the post is replying to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the posts likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the comments of the posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the likes for each comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the replies for each comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the commenter (for each comment or reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the commenter gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the posts a user reacted to or commented to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a list of the users commented or reacted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
