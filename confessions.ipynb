{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import bs4\n",
    "import os\n",
    "import threading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set SCRAPE=True in order to download the data from FB\n",
    "### Set REDUCE=True to reduce the separated json files to one\n",
    "### SCRAPE and REDUCE must not be both set to True or the script will crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCRAPE = True\n",
    "REDUCE = False\n",
    "\n",
    "UNIFIED_JSON_PATH = \"posts.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to return an empty list if string is not JSON formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    try:\n",
    "        return json.loads(open(path, \"r\").read())\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        return []\n",
    "\n",
    "def json_append(path, post):\n",
    "    data = read_json(path)\n",
    "    data.append(post)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(json.dumps(data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a supplemental function to simplify API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get(url_):\n",
    "    return json.loads(requests.get(url_).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define gruop ID variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TECHNION_CONFESSIONS_ID = \"134517547222780\"\n",
    "TAU_CONFESSIONS_ID = \"561380070875128\"\n",
    "IDC_CONFESSIONS_ID = \"199527394120566\"\n",
    "HUJI_CONFESSIONS_ID = \"323288791493138\"\n",
    "BGU_CONFESSIONS_ID = \"151003595697352\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to Graph API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TOKEN = \"EAACEdEose0cBAEIYEOmJ51kfOHPinwL2ychqVpFT0D9ezMjfe1pMKyH0lP2ZA9RuZBH3wQeJ0EfEboHEqDqy7Gq8PSjIcHgNv1lRyC3ZBMk0CdbdzWckHsqNTRatwPZBvkMBfic2P1jwg5vZAZCHKbHdvFfMjjsHGjeVsY7AqEbpWiUBEb0d1zsKJgJwQw3wTWs6ZBb4WSOOQZDZD\"\n",
    "\n",
    "s = \"6cc937f2a9dbc9df92600f365c777d1a\"\n",
    "i = \"652869818252649\"\n",
    "u = \"https://graph.facebook.com/oauth/access_token?client_id={id}&client_secret={secret}&grant_type=client_credentials\"\n",
    "\n",
    "TOKEN = get(u.format(id=i, secret=s))['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Graph API host and API node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOST = \"https://graph.facebook.com/\"\n",
    "API_NODE = \"v2.12/\"\n",
    "QUERY = \"?fields=created_time,message_tags,message,shares,reactions.type(LIKE).limit(0).summary(1).as(like),reactions.type(LOVE).limit(0).summary(1).as(love),reactions.type(HAHA).limit(0).summary(1).as(haha),reactions.type(WOW).limit(0).summary(1).as(wow),reactions.type(SAD).limit(0).summary(1).as(sad),reactions.type(ANGRY).limit(0).summary(1).as(angry)&limit=10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of the pages that are going to be scraped in a manner of (id, alias) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAGES = [\n",
    "    (TECHNION_CONFESSIONS_ID, \"TECHNION\"),\n",
    "    (TAU_CONFESSIONS_ID, \"TAU\"),\n",
    "    (IDC_CONFESSIONS_ID, \"IDC\"),\n",
    "    (HUJI_CONFESSIONS_ID, \"HUJI\"),\n",
    "    (BGU_CONFESSIONS_ID, \"BGU\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a destination file for each page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, name in PAGES:\n",
    "    json_name = \"{}.json\".format(name)\n",
    "    f = open(json_name, \"w\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a supplemental builder functions for the API calls' URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_url(page_id):\n",
    "    return HOST + API_NODE + page_id + \"/posts\" + QUERY + \"&access_token={}\".format(TOKEN)\n",
    "\n",
    "def build_comments_url(post_id):\n",
    "    return HOST + API_NODE + post_id + \"/comments\" + QUERY + \"&access_token={}\".format(TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that scrapes a general object (post or comment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_object(raw_object, object_type, object_source, parent_id=None):\n",
    "    obj = {}   \n",
    "    \n",
    "    obj['type'] = object_type\n",
    "    obj['source'] = object_source\n",
    "    \n",
    "    obj['id'] = raw_object['id']\n",
    "    obj[parent_id] = parent_id\n",
    "    \n",
    "    try:\n",
    "        obj['message'] = raw_object['message']\n",
    "    except KeyError:\n",
    "        obj['message'] = ''\n",
    "\n",
    "    obj['created_time'] = raw_object['created_time']\n",
    "\n",
    "    like = raw_object[\"like\"][\"summary\"][\"total_count\"]\n",
    "    love = raw_object[\"love\"][\"summary\"][\"total_count\"]\n",
    "    haha = raw_object[\"haha\"][\"summary\"][\"total_count\"]\n",
    "    wow = raw_object[\"wow\"][\"summary\"][\"total_count\"]\n",
    "    sad = raw_object[\"sad\"][\"summary\"][\"total_count\"]\n",
    "    angry = raw_object[\"angry\"][\"summary\"][\"total_count\"]\n",
    "\n",
    "    obj['like'] = like\n",
    "    obj['love'] = love\n",
    "    obj['haha'] = haha\n",
    "    obj['wow'] = wow\n",
    "    obj['sad'] = sad\n",
    "    obj['angry'] = angry\n",
    "    \n",
    "    obj['total_reactions'] = sum([like, love, haha, wow, sad, angry])\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a fumction that will paginate through the posts of a URL and scrape them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posts(url, source):\n",
    "    json_path = \"{}.json\".format(source)\n",
    "    res = get(url)\n",
    "    if 'data' in res:\n",
    "        for raw_post in res['data']:\n",
    "            post = query_object(raw_post, \"POST\", source)\n",
    "            json_append(json_path, post)\n",
    "    if 'paging' in res:\n",
    "        if 'next' in res['paging']:\n",
    "            return get_posts(res['paging']['next'], source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will scrape a tag of a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_tag(raw_tag, comment_id, source):\n",
    "    tag = {}\n",
    "    \n",
    "    tag[\"type\"] = \"TAG\"\n",
    "    tag[\"parent_id\"] = comment_id\n",
    "    tag[\"source\"] = source\n",
    "    \n",
    "    tag[\"user_id\"] = raw_tag[\"id\"]\n",
    "    tag[\"user_name\"] = raw_tag[\"name\"]\n",
    "    \n",
    "    return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will call the API for each post comments, scrape them and get their tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_post_comments(url, post_id, post_source):\n",
    "    json_path = \"{}.json\".format(post_source)\n",
    "    res = get(url)\n",
    "    if 'data' in res:\n",
    "        for raw_comment in res['data']:\n",
    "            comment = query_object(raw_comment, \"COMMENT\", post_source, parent_id=post_id)\n",
    "            json_append(json_path, comment)\n",
    "            \n",
    "            if raw_comment.get(\"message_tags\"):\n",
    "                for raw_tag in raw_comment.get(\"message_tags\"):\n",
    "                    tag = parse_tag(raw_tag, raw_comment[\"id\"], post_source)\n",
    "                    json_append(json_path, tag)\n",
    "            \n",
    "    if 'paging' in res:\n",
    "        if 'next' in res['paging']:\n",
    "            return get_post_comments(res['paging']['next'], post_id, post_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, create a function that will scrapes all of the posts' comments and comment tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comments(path):\n",
    "    for post in read_json(path):\n",
    "        get_post_comments(build_comments_url(post[\"id\"]), post[\"id\"], post[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use threading to query each Facebook Group in a different thread - this is the worker funciton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_scraper_worker(node, name):\n",
    "    get_posts(build_url(node), name)\n",
    "    get_comments(\"{}.json\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If SCRAPE, the script will run the threads (stop them using >>taskkill /f /im -\"python.exe\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if SCRAPE:\n",
    "    threads = []\n",
    "    for node, name in PAGES:\n",
    "        t = threading.Thread(target=group_scraper_worker, args=(node, name,))\n",
    "        threads.append(t)\n",
    "        t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If REDUCE, concatenate all the groups JSON files to one, and save it as \"posts.json\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if REDUCE:\n",
    "    jsons = [read_json(\"{}.json\".format(src)) for i, src in PAGES]\n",
    "    unified_json = reduce(lambda x, y: x + y, jsons)\n",
    "    \n",
    "    with open(UNIFIED_JSON_PATH, \"w\") as f:\n",
    "        f.write(json.dumps(unified_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the head of the posts table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_json(open(\"posts.json\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ID_REGEX = \"(#\\d+|\\d+#){1}\"\n",
    "# df[\"id\"] = df[\"message\"].str.findall(ID_REGEX).str.get(0)\n",
    "# df[\"reply_to\"] = df[\"message\"].str.findall(ID_REGEX).str.get(1)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\connection.py\", line 141, in _new_conn\n",
      "    (self.host, self.port), self.timeout, **extra_kw)\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 83, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 601, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 346, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 850, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\connection.py\", line 284, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\connection.py\", line 150, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e)\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x04CC5630>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python3\\lib\\site-packages\\requests\\adapters.py\", line 440, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 639, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"C:\\Python3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 388, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='graph.facebook.com', port=443): Max retries exceeded with url: /v2.12/134517547222780_144915296183005/comments?access_token=652869818252649%7C5nDbBpCHDA_0y9XyM1YsGN7ZiQQ&fields=created_time%2Cmessage_tags%2Cmessage%2Cshares%2Creactions.type%28LIKE%29.limit%280%29.summary%281%29.as%28like%29%2Creactions.type%28LOVE%29.limit%280%29.summary%281%29.as%28love%29%2Creactions.type%28HAHA%29.limit%280%29.summary%281%29.as%28haha%29%2Creactions.type%28WOW%29.limit%280%29.summary%281%29.as%28wow%29%2Creactions.type%28SAD%29.limit%280%29.summary%281%29.as%28sad%29%2Creactions.type%28ANGRY%29.limit%280%29.summary%281%29.as%28angry%29&limit=10&after=WTI5dGJXVnVkRjlqZAFhKemIzSTZANVFEwT1RNeU9EQXlPRFEzT1RJeE9qRTFNVGd3T1RJd016TT0ZD (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x04CC5630>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Python3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-58-eea906380b34>\", line 3, in group_scraper_worker\n",
      "    get_comments(\"{}.json\".format(name))\n",
      "  File \"<ipython-input-57-c07faf68dd59>\", line 3, in get_comments\n",
      "    get_post_comments(build_comments_url(post[\"id\"]), post[\"id\"], post[\"source\"])\n",
      "  File \"<ipython-input-56-0f2ab8455c38>\", line 16, in get_post_comments\n",
      "    return get_post_comments(res['paging']['next'], post_id, post_source)\n",
      "  File \"<ipython-input-56-0f2ab8455c38>\", line 3, in get_post_comments\n",
      "    res = get(url)\n",
      "  File \"<ipython-input-46-183a6e4ee947>\", line 2, in get\n",
      "    return json.loads(requests.get(url_).text)\n",
      "  File \"C:\\Python3\\lib\\site-packages\\requests\\api.py\", line 72, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Python3\\lib\\site-packages\\requests\\api.py\", line 58, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Python3\\lib\\site-packages\\requests\\sessions.py\", line 508, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Python3\\lib\\site-packages\\requests\\sessions.py\", line 618, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Python3\\lib\\site-packages\\requests\\adapters.py\", line 508, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='graph.facebook.com', port=443): Max retries exceeded with url: /v2.12/134517547222780_144915296183005/comments?access_token=652869818252649%7C5nDbBpCHDA_0y9XyM1YsGN7ZiQQ&fields=created_time%2Cmessage_tags%2Cmessage%2Cshares%2Creactions.type%28LIKE%29.limit%280%29.summary%281%29.as%28like%29%2Creactions.type%28LOVE%29.limit%280%29.summary%281%29.as%28love%29%2Creactions.type%28HAHA%29.limit%280%29.summary%281%29.as%28haha%29%2Creactions.type%28WOW%29.limit%280%29.summary%281%29.as%28wow%29%2Creactions.type%28SAD%29.limit%280%29.summary%281%29.as%28sad%29%2Creactions.type%28ANGRY%29.limit%280%29.summary%281%29.as%28angry%29&limit=10&after=WTI5dGJXVnVkRjlqZAFhKemIzSTZANVFEwT1RNeU9EQXlPRFEzT1RJeE9qRTFNVGd3T1RJd016TT0ZD (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x04CC5630>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond',))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.groupby([\"source\"]).agg([\"count\"])[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ID of the posts that the post is replying to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the posts likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the comments of the posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the likes for each comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the replies for each comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the commenter (for each comment or reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the commenter gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the posts a user reacted to or commented to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a list of the users commented or reacted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
