{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import bs4\n",
    "import os\n",
    "import sqlite3\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set SCRAPE=True in order to download the data from FB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCRAPE = True\n",
    "\n",
    "UNIFIED_JSON_PATH = \"posts.json\"\n",
    "LOGGING_INTERVAL = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to return an empty list if string is not JSON formatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    try:\n",
    "        return json.loads(open(path, \"r\").read())\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        return []\n",
    "\n",
    "def json_append(path, post):\n",
    "    data = read_json(path)\n",
    "    data.append(post)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a supplemental function to simplify API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get(url_):\n",
    "    return json.loads(requests.get(url_).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define gruop ID variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TECHNION_CONFESSIONS_ID = \"134517547222780\"   # Technion - Haifa\n",
    "TAU_CONFESSIONS_ID = \"561380070875128\"        # Tel Aviv University\n",
    "IDC_CONFESSIONS_ID = \"199527394120566\"        # Beintchumi - Herzlia\n",
    "HUJI_CONFESSIONS_ID = \"323288791493138\"       # Hebrew University - Jerusalem\n",
    "BGU_CONFESSIONS_ID = \"151003595697352\"        # Ben Gurion University - Beer Sheva\n",
    "ARIEL_CONFESSIONS_ID = \"465435713853175\"      # Ariel University\n",
    "HAIFA_CONFESSIONS_ID = \"417201845375921\"      # Haifa University\n",
    "HADASSAH_CONFESSIONS_ID = \"2044836585541512\"  # Hadassah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to Graph API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TOKEN = \"EAACEdEose0cBAEIYEOmJ51kfOHPinwL2ychqVpFT0D9ezMjfe1pMKyH0lP2ZA9RuZBH3wQeJ0EfEboHEqDqy7Gq8PSjIcHgNv1lRyC3ZBMk0CdbdzWckHsqNTRatwPZBvkMBfic2P1jwg5vZAZCHKbHdvFfMjjsHGjeVsY7AqEbpWiUBEb0d1zsKJgJwQw3wTWs6ZBb4WSOOQZDZD\"\n",
    "\n",
    "s = \"6cc937f2a9dbc9df92600f365c777d1a\"\n",
    "i = \"652869818252649\"\n",
    "u = \"https://graph.facebook.com/oauth/access_token?client_id={id}&client_secret={secret}&grant_type=client_credentials\"\n",
    "\n",
    "TOKEN = get(u.format(id=i, secret=s))['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Graph API host and API node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOST = \"https://graph.facebook.com/\"\n",
    "API_NODE = \"v2.12/\"\n",
    "QUERY = \"?fields=created_time,message_tags,message,shares,reactions.type(LIKE).limit(0).summary(1).as(like),reactions.type(LOVE).limit(0).summary(1).as(love),reactions.type(HAHA).limit(0).summary(1).as(haha),reactions.type(WOW).limit(0).summary(1).as(wow),reactions.type(SAD).limit(0).summary(1).as(sad),reactions.type(ANGRY).limit(0).summary(1).as(angry)&limit=10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of the pages that are going to be scraped in a manner of (id, alias) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAGES = [\n",
    "    (TECHNION_CONFESSIONS_ID, \"TECHNION\"),\n",
    "    (TAU_CONFESSIONS_ID, \"TAU\"),\n",
    "    (IDC_CONFESSIONS_ID, \"IDC\"),\n",
    "    (HUJI_CONFESSIONS_ID, \"HUJI\"),\n",
    "    (BGU_CONFESSIONS_ID, \"BGU\"),\n",
    "    (ARIEL_CONFESSIONS_ID, \"ARIEL\"),\n",
    "    (HAIFA_CONFESSIONS_ID, \"HAIFA\"),\n",
    "    (HADASSAH_CONFESSIONS_ID, \"HADASSAH\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Container for the scraped posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = []\n",
    "last_pages = {}  # This is intended for adding more posts without calling for scraped pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a supplemental builder functions for the API calls' URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_url(page_id):\n",
    "    return HOST + API_NODE + page_id + \"/posts\" + QUERY + \"&access_token={}\".format(TOKEN)\n",
    "\n",
    "def build_comments_url(post_id):\n",
    "    return HOST + API_NODE + post_id + \"/comments\" + QUERY + \"&access_token={}\".format(TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that scrapes a general object (post or comment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_object(raw_object, object_type, object_source, parent_id=None):\n",
    "    obj = {}   \n",
    "    \n",
    "    obj['type'] = object_type\n",
    "    obj['source'] = object_source\n",
    "    \n",
    "    obj['id'] = raw_object['id']\n",
    "    obj['parent_id'] = parent_id\n",
    "    \n",
    "    try:\n",
    "        obj['message'] = raw_object['message']\n",
    "    except KeyError:\n",
    "        obj['message'] = ''\n",
    "\n",
    "    obj['created_time'] = raw_object['created_time']\n",
    "\n",
    "    like = raw_object[\"like\"][\"summary\"][\"total_count\"]\n",
    "    love = raw_object[\"love\"][\"summary\"][\"total_count\"]\n",
    "    haha = raw_object[\"haha\"][\"summary\"][\"total_count\"]\n",
    "    wow = raw_object[\"wow\"][\"summary\"][\"total_count\"]\n",
    "    sad = raw_object[\"sad\"][\"summary\"][\"total_count\"]\n",
    "    angry = raw_object[\"angry\"][\"summary\"][\"total_count\"]\n",
    "\n",
    "    obj['like'] = like\n",
    "    obj['love'] = love\n",
    "    obj['haha'] = haha\n",
    "    obj['wow'] = wow\n",
    "    obj['sad'] = sad\n",
    "    obj['angry'] = angry\n",
    "    \n",
    "    obj['total_reactions'] = sum([like, love, haha, wow, sad, angry])\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a fumction that will paginate through the posts of a URL and scrape them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_posts(url, source):\n",
    "    res = get(url)\n",
    "    if 'data' in res:\n",
    "        for i, raw_post in enumerate(res['data']):\n",
    "            post = query_object(raw_post, \"POST\", source)\n",
    "            posts.append(post)\n",
    "            if len(posts) % LOGGING_INTERVAL == 0:\n",
    "                print(\"{} is on item #{}\".format(source, len(posts)))\n",
    "    if 'paging' in res:\n",
    "        if 'next' in res['paging']:\n",
    "            last_pages[source] = url\n",
    "            return get_posts(res['paging']['next'], source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will scrape a tag of a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_tag(raw_tag, comment_id, source):\n",
    "    tag = {}\n",
    "    \n",
    "    tag[\"type\"] = \"TAG\"\n",
    "    tag[\"parent_id\"] = comment_id\n",
    "    tag[\"source\"] = source\n",
    "    \n",
    "    try:\n",
    "        tag[\"user_id\"] = raw_tag[\"id\"]\n",
    "    except KeyError:\n",
    "        tag[\"user_id\"] = None\n",
    "    \n",
    "    try:\n",
    "        tag[\"user_name\"] = raw_tag[\"name\"]\n",
    "    except KeyError:\n",
    "        tag[\"user_name\"] = None\n",
    "    \n",
    "    return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will call the API for each post comments, scrape them and get their tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_post_comments(url, post_id, post_source):\n",
    "    json_path = \"{}.json\".format(post_source)\n",
    "    res = get(url)\n",
    "    if 'data' in res:\n",
    "        for raw_comment in res['data']:\n",
    "            comment = query_object(raw_comment, \"COMMENT\", post_source, parent_id=post_id)\n",
    "            posts.append(comment)\n",
    "            \n",
    "            if raw_comment.get(\"message_tags\"):\n",
    "                for raw_tag in raw_comment.get(\"message_tags\"):\n",
    "                    tag = parse_tag(raw_tag, raw_comment[\"id\"], post_source)\n",
    "                    posts.append(tag)\n",
    "            \n",
    "            if len(posts) % LOGGING_INTERVAL == 0:\n",
    "                print(\"{} is on item #{}\".format(post_source, len(posts)))\n",
    "            \n",
    "    if 'paging' in res:\n",
    "        if 'next' in res['paging']:\n",
    "            return get_post_comments(res['paging']['next'], post_id, post_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, create a function that will scrapes all of the posts' comments and comment tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comments(posts_list):\n",
    "    for post in posts_list:\n",
    "        get_post_comments(build_comments_url(post[\"id\"]), post[\"id\"], post[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use threading to query each Facebook Group in a different thread - this is the worker funciton:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If SCRAPE, the script will run the threads (stop them using >>taskkill /f /im -\"python.exe\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAU is on item #150\n",
      "TAU is on item #300\n",
      "TAU is on item #450\n",
      "TAU is on item #600\n",
      "HUJI is on item #750\n",
      "HUJI is on item #900\n",
      "TAU is on item #1050\n",
      "IDC is on item #1200\n",
      "IDC is on item #1350\n",
      "TAU is on item #1500\n",
      "HAIFA is on item #1650\n",
      "HAIFA is on item #1800\n",
      "TECHNION is on item #1950\n",
      "TECHNION is on item #2100\n",
      "IDC is on item #2250\n",
      "HUJI is on item #2400\n",
      "TECHNION is on item #2550\n",
      "HAIFA is on item #2700\n",
      "HADASSAH is on item #2850\n",
      "BGU is on item #3000\n",
      "HAIFA is on item #3150\n",
      "BGU is on item #3300\n",
      "BGU is on item #3450\n",
      "ARIEL is on item #3600\n",
      "BGU is on item #3750\n",
      "ARIEL is on item #3900\n",
      "HUJI is on item #4050\n",
      "IDC is on item #4200\n",
      "TECHNION is on item #4350\n",
      "ARIEL is on item #4500\n",
      "IDC is on item #4650\n",
      "BGU is on item #4800\n",
      "IDC is on item #4950\n",
      "ARIEL is on item #5100\n",
      "TAU is on item #5250\n",
      "IDC is on item #5400\n",
      "TECHNION is on item #5550\n",
      "TECHNION is on item #5700\n",
      "TECHNION is on item #5850\n",
      "BGU is on item #6000\n",
      "BGU is on item #6150\n"
     ]
    }
   ],
   "source": [
    "if SCRAPE:\n",
    "    \n",
    "    threads = []\n",
    "    \n",
    "    for node, name in PAGES:\n",
    "        t = threading.Thread(target=get_posts, args=(build_url(node), name,))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "        \n",
    "else:\n",
    "    posts = pd.read_csv(\"posts.csv\").to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"posts.json\", \"w\") as f:\n",
    "    f.write(json.dumps(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[II] Scraping comments...\n",
      "TECHNION is on item #6300\n",
      "HUJI is on item #6450\n",
      "TECHNION is on item #6600\n",
      "HAIFA is on item #6750\n",
      "TECHNION is on item #6900\n",
      "ARIEL is on item #7050\n",
      "TAU is on item #7200\n",
      "HUJI is on item #7350\n",
      "TAU is on item #7500\n",
      "IDC is on item #7650\n",
      "IDC is on item #7800\n",
      "BGU is on item #7950\n",
      "BGU is on item #8100\n",
      "HADASSAH is on item #8250\n",
      "ARIEL is on item #8400\n",
      "ARIEL is on item #8550\n",
      "HADASSAH is on item #8700\n",
      "IDC is on item #8850\n",
      "TECHNION is on item #9000\n",
      "IDC is on item #9150\n",
      "BGU is on item #9300\n",
      "ARIEL is on item #9450\n"
     ]
    }
   ],
   "source": [
    "if SCRAPE:\n",
    "    \n",
    "    threads = []\n",
    "    \n",
    "    print(\"[II] Scraping comments...\")\n",
    "    for posts_list in chunks(posts, 20):\n",
    "        t = threading.Thread(target=get_comments, args=(posts_list,))\n",
    "        threads.append(t)\n",
    "        t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_sql(\"posts\", sqlite3.connect(\"data.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angry</th>\n",
       "      <th>created_time</th>\n",
       "      <th>haha</th>\n",
       "      <th>id</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>message</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>sad</th>\n",
       "      <th>source</th>\n",
       "      <th>total_reactions</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>wow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-02-13T18:32:34+0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>465435713853175_470884616641618</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#2439 אחד החברה ניגש למועד ב על 90, שיהנה ויצל...</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>8.0</td>\n",
       "      <td>POST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-02-13T18:31:51+0000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>465435713853175_470884349974978</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#2438 מאז שעברנו לספריה החדשה אני חולמת שיום א...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>36.0</td>\n",
       "      <td>POST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-02-13T18:31:33+0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>465435713853175_470884173308329</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#2437 מתחיל לאבד את זה אחרי ארבעה נכשלים. \\n#ע...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>6.0</td>\n",
       "      <td>POST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-02-13T18:31:21+0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>465435713853175_470884116641668</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#2436 כשאת מודיעה לכל התואר שנכנס הכסף אחרי צב...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3.0</td>\n",
       "      <td>POST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-02-13T18:31:10+0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>465435713853175_470884063308340</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#2435 הצעת חוק: להקצות קומה שלמה בספריה לאנשים...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>16.0</td>\n",
       "      <td>POST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   angry              created_time  haha                               id  \\\n",
       "0    0.0  2018-02-13T18:32:34+0000   0.0  465435713853175_470884616641618   \n",
       "1    0.0  2018-02-13T18:31:51+0000   6.0  465435713853175_470884349974978   \n",
       "2    0.0  2018-02-13T18:31:33+0000   0.0  465435713853175_470884173308329   \n",
       "3    0.0  2018-02-13T18:31:21+0000   0.0  465435713853175_470884116641668   \n",
       "4    0.0  2018-02-13T18:31:10+0000   2.0  465435713853175_470884063308340   \n",
       "\n",
       "   like  love                                            message parent_id  \\\n",
       "0   7.0   0.0  #2439 אחד החברה ניגש למועד ב על 90, שיהנה ויצל...      None   \n",
       "1  30.0   0.0  #2438 מאז שעברנו לספריה החדשה אני חולמת שיום א...      None   \n",
       "2   6.0   0.0  #2437 מתחיל לאבד את זה אחרי ארבעה נכשלים. \\n#ע...      None   \n",
       "3   3.0   0.0  #2436 כשאת מודיעה לכל התואר שנכנס הכסף אחרי צב...      None   \n",
       "4  14.0   0.0  #2435 הצעת חוק: להקצות קומה שלמה בספריה לאנשים...      None   \n",
       "\n",
       "   sad source  total_reactions  type user_id user_name  wow  \n",
       "0  1.0  ARIEL              8.0  POST     NaN       NaN  0.0  \n",
       "1  0.0  ARIEL             36.0  POST     NaN       NaN  0.0  \n",
       "2  0.0  ARIEL              6.0  POST     NaN       NaN  0.0  \n",
       "3  0.0  ARIEL              3.0  POST     NaN       NaN  0.0  \n",
       "4  0.0  ARIEL             16.0  POST     NaN       NaN  0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
